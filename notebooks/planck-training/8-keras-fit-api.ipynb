{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import yaml\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import began\n",
    "from began.logging import setup_vae_run_logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, kernel_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = make_vae_inference_net(latent_dim)\n",
    "        self.generative_net = make_vae_generative_net(latent_dim, kernel_size)\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DustVAEder(object):\n",
    "    \"\"\" Convolutional variational autoencoder model\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameters):\n",
    "        self.hps = hyperparameters\n",
    "        self.model = self._build_model()\n",
    "        self.model.summary()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = Adam(beta_1=self.hps['beta_1'], lr=self.hps['learning_rate'])\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = self._model_loss()\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = \n",
    "        decoder = build_decoder(self.hps['latent_dim'], self.hps['kernel_size'])\n",
    "        encoder = build_encoder(self.hps['latent_dim'])\n",
    "        tf.keras.models.Model()\n",
    "        return\n",
    "\n",
    "    def _model_loss(self):\n",
    "        rec_loss = self._reconstruction_loss()\n",
    "        kl_loss = self._kl_divergence_lsos()\n",
    "\n",
    "        @tf.function\n",
    "        def _total_loss(x_true, x_pred):\n",
    "            rec_loss(x_true, x_pred)\n",
    "            return rec_loss + kl_loss\n",
    "        return _total_loss\n",
    "\n",
    "    def _reconstruction_loss(self):\n",
    "        return\n",
    "\n",
    "    def _kl_divergence_loss(self):\n",
    "        return\n",
    "\n",
    "@tf.function\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\\\n",
    "    \"\"\" Convenience function to calculate lognormal pdf.\n",
    "    \"\"\"\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "def build_decoder(latent_dim, kernel_size):\n",
    "    model = tf.keras.Sequential(name='Decoder')\n",
    "    \n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(latent_dim,)))\n",
    "    model.add(tf.keras.layers.Dense(units=16*16*32, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Reshape(target_shape=(16, 16, 32)))\n",
    "    assert model.output_shape == (None, 16, 16, 32)\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=kernel_size, strides=(2, 2), padding=\"SAME\", activation='relu'))\n",
    "    assert model.output_shape == (None, 32, 32, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=kernel_size, strides=(2, 2), padding=\"SAME\", activation='relu'))\n",
    "    assert model.output_shape == (None, 64, 64, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=kernel_size, strides=(2, 2), padding=\"SAME\", activation='relu'))\n",
    "    assert model.output_shape == (None, 128, 128, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=kernel_size, strides=(2, 2), padding=\"SAME\", activation='relu'))\n",
    "    assert model.output_shape == (None, 256, 256, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=kernel_size, strides=(1, 1), padding=\"SAME\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_encoder(latent_dim):\n",
    "    model = tf.keras.Sequential(name='Encoder')\n",
    "    \n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(256, 256, 1)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 128, 128, 256)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 64, 64, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=1, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    assert model.output_shape == (None, 32 * 32 * 64)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(latent_dim + latent_dim))\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed in numpy\n",
    "np.random.seed(123454321)\n",
    "# initialize random seed in tensorflow\n",
    "tf.random.set_seed(123454321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Batch and shuffle the data\n",
    "with h5py.File(\"../data/preprocessed/prepared.h5\", 'r') as f:\n",
    "    dset = f[\"cut_maps\"]\n",
    "    train_images = dset[...].astype(np.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).map(tf.image.per_image_standardization)\n",
    "test_dataset = dataset.take(100)\n",
    "train_dataset = dataset.skip(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"Builds a convolutional model.\"\"\"\n",
    "    lat_dim = hp.Int('lat_dim', 32, 256)\n",
    "    kernel_size = hp.Choice('kernel_size', values=[3, 5])\n",
    "    return began.CVAE(lat_dim, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, train_ds):\n",
    "        hp = trial.hyperparameters\n",
    "\n",
    "        # Hyperparameters can be added anywhere inside `run_trial`.\n",
    "        # When the first trial is run, they will take on their default values.\n",
    "        # Afterwards, they will be tuned by the `Oracle`.\n",
    "        train_ds = train_ds.batch(hp.Int('batch_size', 8, 32, default=8))\n",
    "        print(type(train_ds))\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
    "        optimizer = tf.keras.optimizers.Adam(beta_1=0.5, learning_rate=lr)\n",
    "        epoch_loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "        @tf.function\n",
    "        def run_train_step(data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = began.vae.compute_loss(model, data)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            epoch_loss_metric.update_state(loss)\n",
    "            return loss\n",
    "\n",
    "        # `self.on_epoch_end` reports results to the `Oracle` and saves the\n",
    "        # current state of the Model. The other hooks called here only log values\n",
    "        # for display but can also be overridden. For use cases where there is no\n",
    "        # natural concept of epoch, you do not have to call any of these hooks. In\n",
    "        # this case you should instead call `self.oracle.update_trial` and\n",
    "        # `self.oracle.save_model` manually.\n",
    "        for epoch in range(3):\n",
    "            print('Epoch: {}'.format(epoch))\n",
    "\n",
    "            self.on_epoch_begin(trial, model, epoch, logs={})\n",
    "            for batch, data in enumerate(train_ds):\n",
    "                self.on_batch_begin(trial, model, batch, logs={})\n",
    "                batch_loss = float(run_train_step(data))\n",
    "                self.on_batch_end(trial, model, batch, logs={'loss': batch_loss})\n",
    "\n",
    "                if batch % 30 == 0:\n",
    "                    loss = epoch_loss_metric.result().numpy()\n",
    "                    print('Batch: {}, Average Loss: {}'.format(batch, loss))\n",
    "\n",
    "            epoch_loss = epoch_loss_metric.result().numpy()\n",
    "            self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss})\n",
    "            epoch_loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project results/began_custom_training_hp/oracle.json\n",
      "INFO:tensorflow:Reloading Oracle from existing project results/began_custom_training_hp/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from results/began_custom_training_hp/tuner0.json\n",
      "INFO:tensorflow:Reloading Tuner from results/began_custom_training_hp/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = MyTuner(\n",
    "      oracle=kt.oracles.BayesianOptimization(\n",
    "          objective=kt.Objective('loss', 'min'),\n",
    "          max_trials=10),\n",
    "      hypermodel=build_model,\n",
    "      directory='results',\n",
    "      project_name='began_custom_training_hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat_dim': 131, 'kernel_size': 5, 'batch_size': 10, 'learning_rate': 0.00019071203207567507}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
