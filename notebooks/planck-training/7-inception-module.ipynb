{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import yaml\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "import began\n",
    "from began.logging import setup_vae_run_logging\n",
    "from began.vae import inception_module\n",
    "from began.visualization import mplot\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D, Concatenate, Dense, Reshape, Input\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating a naive inception block\n",
    "def inception_module(layer_in, f1, f2, f3):\n",
    "    \"\"\" An implementation of the Inception module.\n",
    "    \"\"\"\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = Concatenate(axis=-1)([conv1, conv3, conv5, pool])\n",
    "    return layer_out\n",
    "\n",
    "# function for creating a naive inception block\n",
    "def transpose_inception_module(layer_in, f1, f2, f3):\n",
    "    \"\"\" An implementation of the Inception module.\n",
    "    \"\"\"\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2DTranspose(f1, kernel_size=1, strides=(2, 2), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2DTranspose(f2, kernel_size=1, strides=(2, 2), padding='same', activation='relu')(layer_in)\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2DTranspose(f3, kernel_size=1, strides=(2, 2), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 max pooling\n",
    "    upsamp = UpSampling2D((2, 2), interpolation='nearest')(layer_in)\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(upsamp)\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = Concatenate(axis=-1)([conv1, conv3, conv5, pool])\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 128)       102528    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 128)     409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 256, 256, 128)     409728    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 256, 256, 1)       3201      \n",
      "=================================================================\n",
      "Total params: 2,393,345\n",
      "Trainable params: 2,392,513\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lat_dim = 128\n",
    "latz = Input((lat_dim,))\n",
    "\n",
    "dlr0 = Dense(units=(16 * 16 * 32), activation=tf.nn.relu)(latz)\n",
    "res0 = Reshape(target_shape=(16, 16, 32))(dlr0)\n",
    "assert res0.shape.as_list() == [None, 16, 16, 32]\n",
    "\n",
    "btn0 = BatchNormalization(momentum=0.9)(res0)\n",
    "cvt0 = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"SAME\", activation='relu')(btn0)\n",
    "assert cvt0.shape.as_list() == [None, 32, 32, 128]\n",
    "\n",
    "btn1 = BatchNormalization(momentum=0.9)(cvt0)\n",
    "cvt1 = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"SAME\", activation='relu')(btn1)\n",
    "assert cvt1.shape.as_list() == [None, 64, 64, 128]\n",
    "\n",
    "btn2 = BatchNormalization(momentum=0.9)(cvt1)\n",
    "cvt2 = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"SAME\", activation='relu')(btn2)\n",
    "assert cvt2.shape.as_list() == [None, 128, 128, 128]\n",
    "\n",
    "btn3 = BatchNormalization(momentum=0.9)(cvt2)\n",
    "cvt3 = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"SAME\", activation='relu')(btn3)\n",
    "assert cvt3.shape.as_list() == [None, 256, 256, 128]\n",
    "\n",
    "cvt4 = Conv2DTranspose(filters=1, kernel_size=5, strides=(1, 1), padding=\"SAME\")(cvt3)\n",
    "\n",
    "model = Model([latz], cvt4)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(lat_dim=128):\n",
    "    lat_dim = 128\n",
    "    latz = Input((lat_dim,))\n",
    "\n",
    "    dlr0 = Dense(units=(16 * 16 * 32), activation=tf.nn.relu)(latz)\n",
    "    res0 = Reshape(target_shape=(16, 16, 32))(dlr0)\n",
    "    assert res0.shape.as_list() == [None, 16, 16, 32]\n",
    "\n",
    "    btn0 = BatchNormalization(momentum=0.9)(res0)\n",
    "    icp0 = inception_module(btn0, 32, 128, 64)\n",
    "    ups0 = UpSampling2D((2, 2), interpolation='nearest')(icp0)\n",
    "    assert ups0.shape.as_list() == [None, 32, 32, 256]\n",
    "\n",
    "    btn1 = BatchNormalization(momentum=0.9)(ups0)\n",
    "    icp1 = inception_module(btn1, 32, 128, 64)\n",
    "    ups1 = UpSampling2D((2, 2), interpolation='nearest')(icp1)\n",
    "    assert ups1.shape.as_list() == [None, 64, 64, 480]\n",
    "\n",
    "    btn2 = BatchNormalization(momentum=0.9)(ups1)\n",
    "    icp2 = inception_module(btn2, 32, 128, 64)\n",
    "    ups2 = UpSampling2D((2, 2), interpolation='nearest')(icp2)\n",
    "    assert ups2.shape.as_list() == [None, 128, 128, 704]\n",
    "\n",
    "    btn3 = BatchNormalization(momentum=0.9)(ups2)\n",
    "    icp3 = inception_module(btn3, 32, 128, 64)\n",
    "    ups3 = UpSampling2D((2, 2), interpolation='nearest')(icp3)\n",
    "    assert ups3.shape.as_list() == [None, 256, 256, 928]\n",
    "\n",
    "    avg0 = Conv2D(1, (1, 1), padding='same', activation='relu')(ups3)\n",
    "    assert avg0.shape.as_list() == [None, 256, 256, 1]\n",
    "\n",
    "    model = Model([latz], avg0, name=\"Decoder\")\n",
    "    return model\n",
    "\n",
    "def transpose_decoder(lat_dim=128):\n",
    "    lat_dim = 128\n",
    "    latz = Input((lat_dim,))\n",
    "\n",
    "    dlr0 = Dense(units=(16 * 16 * 32), activation=tf.nn.relu)(latz)\n",
    "    res0 = Reshape(target_shape=(16, 16, 32))(dlr0)\n",
    "    assert res0.shape.as_list() == [None, 16, 16, 32]\n",
    "\n",
    "    btn0 = BatchNormalization(momentum=0.9)(res0)\n",
    "    icp0 = transpose_inception_module(btn0, 32, 128, 64)\n",
    "    assert icp0.shape.as_list() == [None, 32, 32, 256]\n",
    "\n",
    "    btn1 = BatchNormalization(momentum=0.9)(icp0)\n",
    "    icp1 = transpose_inception_module(btn1, 32, 128, 64)\n",
    "    assert icp1.shape.as_list() == [None, 64, 64, 480]\n",
    "\n",
    "    btn2 = BatchNormalization(momentum=0.9)(icp1)\n",
    "    icp2 = transpose_inception_module(btn2, 32, 128, 64)\n",
    "    assert icp2.shape.as_list() == [None, 128, 128, 704]\n",
    "\n",
    "    btn3 = BatchNormalization(momentum=0.9)(icp2)\n",
    "    icp3 = transpose_inception_module(btn3, 32, 128, 64)\n",
    "    assert icp3.shape.as_list() == [None, 256, 256, 928]\n",
    "\n",
    "    avg0 = Conv2D(1, (1, 1), padding='same', activation='relu')(icp3)\n",
    "    assert avg0.shape.as_list() == [None, 256, 256, 1]\n",
    "\n",
    "    model = Model([latz], avg0, name=\"Decoder\")\n",
    "    return model\n",
    "\n",
    "def make_vae_inference_net(latent_dim):\n",
    "    model = tf.keras.Sequential(name='Encoder')\n",
    "    \n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(256, 256, 1)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 128, 128, 256)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 64, 64, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=1, padding=\"SAME\", strides=(2, 2), activation='relu'))\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    assert model.output_shape == (None, 32 * 32 * 64)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(latent_dim + latent_dim))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, kernel_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = make_vae_inference_net(latent_dim)\n",
    "        self.generative_net = transpose_decoder(latent_dim)\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    \n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = began.vae.compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input, plot_dir, title=None):\n",
    "    predictions = model.sample(test_input)\n",
    "    mean = np.mean(predictions)\n",
    "    std = np.std(predictions)\n",
    "    fig, _ = mplot(predictions[..., 0], extent=(-10, 10, -10, 10), title=title, cbar_range=[mean - 2*std, mean+2*std])\n",
    "    fig.savefig(plot_dir / 'image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(128, 5)\n",
    "summary_writer = setup_vae_run_logging(128, 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<function Kernel.enter_eventloop.<locals>.advance_eventloop at 0x2aebd545f680>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 310, in advance_eventloop\n",
      "    eventloop(self)\n",
      "TypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../data/preprocessed/prepared.h5\", 'r') as f:\n",
    "    dset = f[\"cut_maps\"]\n",
    "    train_images = dset[...].astype(np.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).batch(8).map(tf.image.per_image_standardization)\n",
    "test_dataset = dataset.take(100)\n",
    "train_dataset = dataset.skip(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[8,928,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradients/Decoder/conv2d_3/Conv2D_grad/Conv2DBackpropInput (defined at /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference___backward_compute_loss_2705_3297]\n\nFunction call stack:\n__backward_compute_loss_2705\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a89d82ca8e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcompute_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dd8b2be08343>\u001b[0m in \u001b[0;36mcompute_apply_gradients\u001b[0;34m(model, x, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    909\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       return self._backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m--> 911\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/projects/gan/began/envs-gpu/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[8,928,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradients/Decoder/conv2d_3/Conv2D_grad/Conv2DBackpropInput (defined at /home/bthorne/projects/gan/began/envs-gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference___backward_compute_loss_2705_3297]\n\nFunction call stack:\n__backward_compute_loss_2705\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(beta_1=0.5, learning_rate=0.0002)\n",
    "random_vector_for_generation = tf.random.normal(shape=[9, 128])\n",
    "for epoch in range(1, 10):\n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    for step, train_x in enumerate(train_dataset):\n",
    "        compute_apply_gradients(model, train_x, optimizer)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "        for test_x in test_dataset:\n",
    "            loss(began.vae.compute_loss(model, test_x))\n",
    "        elbo = -loss.result()\n",
    "        print(\"\\t loss: \", elbo)\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('elbo', elbo, step=epoch)\n",
    "        title = 'Epoch: {:03d}, Test set ELBO: {:04.02f}'.format(epoch, elbo)\n",
    "        generate_and_save_images(model, epoch, random_vector_for_generation, Path(\"./plots\").absolute(), title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mplot(model.decode(np.random.randn(3, 128))[..., 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
